# Проект Stenza — русскоязычные фанфики по «Доктору Кто»

[Сайт](https://stenza.fikl.ru)

## Инструкция пользователя

Сайт поддерживает четыре вида поиска:

* По словоформе: для такого поиска надо ввести в поле поиска интересующую словоформу в кавычках. Например: `"Доктор"`
* По лемме без специфицированной части речи: для такого поиска надо ввести в поисковую строку слово без кавычек в любой его форме. Например: `Роза`
* По частеречному тегу: для этого надо ввести тег в формате UD (заглавными буквами). Например: `NOUN` (существительное)
* По лемме со специфицированной частью речи: для такого поиска надо ввести в поисковую строку запрос вида `слово+часть_речи`. При этом на позицию слова необходимо вводить начальную форму интересующего слова. Например: `кошка+NOUN`. **НО НЕ `кошкой+NOUN`**

Также можно искать N-граммы любой длины, разделяя токены пробелом. Каждый токен можно задавать одним из четырёх вышеописанных способов. 

Например: `не знать что`; `кошка+NOUN VERB`; `NOUN VERB`

Ограничения по длине поискового запроса нет.

### Список наших тегов:

* `NOUN` — существительное;
* `ADJ` — прилагательное;
* `VERB` — глагол;
* `ADV` — наречие;
* `PROPN` — имя собственное;
* `INTJ` — междометие;
* `PRON` — местоимение (любой разряд, кроме детерминативов);
* `DET` — определительное местоимение ("этот", "тот" и другие);
* `NUM` — числительное (в т.ч. и цифровая запись);
* `CCONJ` — сочинительный союз;
* `SCONJ` — подчинительный союз;
* `ADP` — предлог;
* `PART` — частица;
* `AUX` — вспомогательный глагол (например, формы глагола "быть").

При разметке мы пользовались тегами [Universal Dependencies](https://universaldependencies.org/u/pos/), в частности, регламентом Taiga.

## Структура проекта

* `app` — собственно сам корпус, который представляет собой Flask-приложение. Сюда же помещается файл с базой данных;
* `corpus_creation` — набор скриптов для создания корпуса: краулинг, морфолоогическая разметка и создание базы данных;
* `stenza.service` и `stenza.nginx` — конфигурационные файлы для запуска сайта на собственном сервере.

## Локальный запуск

1. Склонируйте репозиторий: `git clone https://github.com/Futyn-Maker/stenza_corpus.git`
2. Перейдите в папку с корпусом: `cd stenza_corpus/app`
3. Установите зависимости, их немного: `pip3 install -r requirements.txt`
4. Запустите сайт: `python3 app.py`
5. Ищите сайт на `http://127.0.0.1:5000`

Ниже можно прочитать подробную информацию о разработке проекта.

## О нашем корпусе

Корпус был собран на основе русскоязычных фанфиков по “Доктору Кто”, представленных на сайте https://ru.fanfiktion.net. Данный сайт был выбран как стартовая площадка для данного проекта, потому что, во-первых, он не запрещал автоматический парсинг (в отличие от более популярных ресурсов типа fanfics.me, archiveofourown), во-вторых, там отсутствовала нативная реклама (как, например, на ficbook.net), что существенно облегчило краулинг текстов. Впоследствии мы допускаем возможность расширения нашего корпуса на более большое количество текстов, в том числе с других сайтов. 

Актуальность данного корпуса обусловливается двумя аспектами. Во-первых, он может быть интересен для филологов-исследователей такого феномена современной культуры, как фанфики. Во-вторых, он представляет собой интересную задачу для NLP, так как позволяет протестировать возможности автоматического морфологического анализа на современных невычитанных текстах. В отличие от многих других источников письменных текстов (книги, газеты), тексты фанфиков написаны непрофессиональными писателями и не проходили профессиональную редактуру, что дает нам возможность посмотреть, как автоматические POS-теггеры будут справляться с текстами с орфографическими и речевыми ошибками. Более того, в текстах, написанных на основании какого-то уже существующего художественного произведения, будет много нестандартных имен собственных, названий нестандартных явлений и действий — всё это позволит проверить реакцию автоматического теггера на слова, которые невозможно задать контекстуально.

## Процесс сбора текстов

Тексты были собраны автоматически с помощью библиотек Python3 Requests и BeautifulSoup в виде документов `.txt`-формата. Потом с помощью регулярных выражений они были очищены от “мусора” типа “мягких переносов”. 

При первичном выкачивании мы использовали только те фанфики, которые были написаны в категории “Джен” и имели рейтинг ниже 18+. В процессе отсматривания текстов выяснилось, что (видимо, из-за малой популярности сайта и плохого качества модерации) среди наших фанфиков оказалось много текстов, содержащих 18+ сцены. Поэтому мы провели ручной отсмотр текстов и исключили подавляющее большинство подобных текстов.

## Морфологический парсер и разметка текстов

В настоящий момент есть огромное количество морфологических парсеров, которые позволяют достаточно точно и правильно обрабатывать тексты на русском языке. Тем не менее, из них всех мы остановились на парсере Stanza. Мы сделали это по нескольким причинам. Первой из них выступили ранее проведенные исследования, которые показали, что Stanza превосходит по качеству многие из альтернативных парсеров. Вторым аргументом в пользу Stanza выступило то, что данный парсер использует при разметке UD. Это позволяет в гипотетическом будущем бесконечно расширять наш корпус, свободно добавляя в него тексты на очень разных языках. К тому же, эта система широко используется в научных работах, поэтому использование ее при разметке станет хорошим подспорьем для ученых, которые будут проводить исследования на нашем корпусе. Заметим в скобках, что из всех популярных парсеров, которые используют UD, Stanza при предварительных исследованиях показала самые лучшие результаты. И, в конечном итоге, Stanza отдает разметку в очень удобных для дальнейшей обработки `.conllu`-файлах, которые имеют очень широкое применение в NLP-индустрии.

Руководствуясь данным набором причин, мы остановились на Stanza как на основном морфологическом парсере.

Очистив тексты от типографического и “морального” мусора, мы прогнали их через Stanza и получили выдачу в виде `.conllu`-файлов. Впоследствии мы добавили к ним метаданные: ник автора, название работы и ссылка на нее. Это и стало основанием для нашего корпуса. 

В конечном счете в нашем корпусе 101 текст, 27374 предложения и 228035 токенов.

## База данных

Для организации корпуса мы воспользовались возможностями базы данных SQLite. В нашей базе данных две таблицы: первая из них содержит предложения, а вторая — слова, которые соответствуют этим предложениям. Таблицы связаны с помощью `ID_Sent`, который позволяет идентифицировать слова, которые содержатся в каждом предложении. В таблице “Предложения” хранятся ID, текст предложения, имя автора, название работы и ссылка на нее. В таблице “Слова” хранятся ID, `ID_Sent`, токен, его лемма и часть речи.  Мы проиндексировали текстовые столбцы этой таблицы, чтобы поиск осуществлялся быстрее.

При поиске в БД применяется единая структура запросов: 

```
request = [([entry], search_type)], search_type IN token, lemma, POS, lemma+POS
```

В нашей БД принципиально различаются поиск одного токена и поиск нескольких токенов. 

При поиске одного токена мы ищем интересующие нас единицы по интересующим нас полям в табличке “Слова”, потом забираем из таблицы “Предложения” соответствующие предложения и метадату, убираем дубликаты и отдаем пользователю.

При поиске нескольких токенов мы выбираем из таблички со словами индексы интересующих нас токенов (сначала отдельно), потом проверяем, что внутри одного предложения они идут подряд, и если это так, забираем предложение из таблицы “Предложения” с метадатой и отдаем пользователю.

Также мы написали функцию, которая приводит строковой запрос к описанному выше формату, чтобы режим определялся автоматически, исходя из формата запроса. В случае с режимом, в котором мы ищем по лемме, требуется лемматизировать введённую пользователем словоформу — для этого мы использовали Pymorphy3. Библиотеку Stanza, которую мы использовали для разметки корпуса, здесь использовать было нельзя — она работает слишком медленно, и запрос пользователя обрабатывался бы очень долго. Кроме того, при лемматизации поискового запроса нам не требуется разрешение неоднозначности (просто нет возможности её разрешить на основе одного слова), а потому мы смело можем отдать предпочтение более лёгкому и быстрому решению.

## Flask

Для удобного использования корпуса мы написали Flask-приложение, в котором реализовали приведённый выше алгоритм поиска. Интерфейс очень минималистичный и, главным образом, написан на Bootstrap. Активно используется функционал шаблонов Jinja2. На главной странице присутствует поисковая строка, в которую пользователь вводит запрос — режим поиска определяется автоматически — а в верхнем меню — ссылка на справку по поиску, где описаны форматы запросов. Результаты поиска выдаются в виде списка релевантных предложений. Под каждым — кликабельная ссылка на источник (собственно текст фанфика). Есть также возможность скопировать пример вместе с метаданными в буфер обмена.

## Тестирование корпуса

В процессе тестирования мы проверили корректность работы всех режимов поиска и потестировали некоторые сложные случаи. Опишем некоторые из них. 

* Идентификация Доктора как имени собственного. Одной из самых очевидных задач применительно к подобному корпусу становится корректная идентификация, собственно, того факта, что имя главного героя — Доктор — это имя собственное, а не существительное. В части контекстов Станзе удалось это определить — 86 контекстов размечены именно так. Тем не менее, есть 1894 контекста, где Доктор размечен как существительное, так что Станзе еще есть куда стремиться.
* Поиск по запросам, состоящим из больших букв. Во вселенной “Доктора Кто” существует ТАРДИС — транспортное средство Доктора. Из-за его своеобразного написания и несклоняемости мы решили проверить, как Stanza справилась с ним. Мы выяснили, что в подавляющем большинстве контекстов это название всё-таки размечено как имя собственное — и только иногда как существительное. Тем не менее, именно благодаря этому кейсу мы нашли ошибку в нашем коде: мы проверяли, является ли введенный элемент морфологическим тегом, на основании того, написан ли он большими буквами. Столкнувшись же с таким кейсом, мы отказались от такой проверки, и вхардкодили список морфологических тегов.
* Анализ совсем неконвенциональных названий. Также мы проверили, как Stanza справилась с неконвенциональными названиями: например, Галлифрей — название родной планеты главного героя. Что удивительно, во всех контекстах Станза это поняла и распарсила правильно!
* Спорная лемматизация — *Она* VS. *ее*. Очень интересным кейсом стал кейс *она* VS. *её*. Изначально мы пришли к нему, желая проверить, влияет ли на результат наличие или отсутствие точечек над ё, но появилась штука гораздо более интересная. Мы выяснили, что Stanza по-разному разметила *её* как форму местоименного существительного и *её* как неизменяемое притяжательное местоимение. Леммой в первом случае является *она*, во втором случае — *её*. Это заставило нас задаться вопросом: а как лучше организовать режимы поиска так, чтобы можно было найти именно ту лемму, которую мы ищем? До этого момента мы хотели изменить режим поиска по специфицированной части речи так, чтобы она работала скорее как поиск по лемме: чтобы, например, `кошка+NOUN` и `кошкой+NOUN` были равнозначны. Для этого пришлось бы отдавать запрос в лемматизатор. Но мы поняли, что наличие кейсов типа *её* создаст нам проблему: найти слова, лемма которых похожа на словоформу другого слова, будет очень проблематична. Например, невозможно будет найти все формы слова *столовая*: если мы, например, внесем изменения и запишем `столовая+NOUN`, то лемматизатор вполне себе может отдать лемму *столовый* (указать часть речи для Pymorphy3 перед лемматизацией не представляется возможным) — что сделает невозможным нахождение слов с леммой *столовая*. Таким образом, мы решили, что будет лучше оставить поиск таким, какой он есть, чтобы пользователь мог задать лемму самостоятельно.
* Скорость. Для того, чтобы проверить скорость (у нас всё синхронно, поэтому скорость обработки запросов критична), мы задали запросы, которые предполагали очень большую выдачу: *ADJ NOUN*, *NOUN*. Время обработки составило ~0,2 секунды, что говорит о том, что наш код достаточно оптимизирован.

## Credits

* Алиса Лёзина — краулинг корпуса, чистка текстов, разметка текстов, запись метаданных
* Полина Уткина — составление логики запросов и заполнение базы данных
* Дарья Харламова — выбор корпуса, выбор парсера, чистка текстов, разметка текстов, запись метаданных, оформление сайта, описание проекта
* Андрей Якубой — изготовление фласк-приложения, хостинг, сбор проекта в единое целое, общий Code Review
