{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa36d12f-0a28-4881-9f2a-f32fa015a3ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in c:\\python38\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: emoji in c:\\python38\\lib\\site-packages (from stanza) (2.8.0)\n",
      "Requirement already satisfied: numpy in c:\\python38\\lib\\site-packages (from stanza) (1.24.2)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\python38\\lib\\site-packages (from stanza) (3.17.3)\n",
      "Requirement already satisfied: requests in c:\\python38\\lib\\site-packages (from stanza) (2.26.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\python38\\lib\\site-packages (from stanza) (2.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\python38\\lib\\site-packages (from stanza) (4.64.0)\n",
      "Requirement already satisfied: six>=1.9 in c:\\python38\\lib\\site-packages (from protobuf>=3.15.0->stanza) (1.15.0)\n",
      "Requirement already satisfied: filelock in c:\\python38\\lib\\site-packages (from torch>=1.3.0->stanza) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in c:\\python38\\lib\\site-packages (from torch>=1.3.0->stanza) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\python38\\lib\\site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\python38\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\python38\\lib\\site-packages (from torch>=1.3.0->stanza) (3.0.1)\n",
      "Requirement already satisfied: fsspec in c:\\python38\\lib\\site-packages (from torch>=1.3.0->stanza) (2023.9.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python38\\lib\\site-packages (from requests->stanza) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python38\\lib\\site-packages (from requests->stanza) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python38\\lib\\site-packages (from requests->stanza) (2.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python38\\lib\\site-packages (from requests->stanza) (3.2)\n",
      "Requirement already satisfied: colorama in c:\\python38\\lib\\site-packages (from tqdm->stanza) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python38\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python38\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a05fcbb1-d4a1-4e9f-b51d-441fe4ad20f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 15.3MB/s]\n",
      "2023-10-21 19:53:47 INFO: Downloading default packages for language: ru (Russian) ...\n",
      "2023-10-21 19:53:48 INFO: File exists: C:\\Users\\GL_24_12_2019\\stanza_resources\\ru\\default.zip\n",
      "2023-10-21 19:53:52 INFO: Finished downloading models and saved to C:\\Users\\GL_24_12_2019\\stanza_resources.\n",
      "2023-10-21 19:53:52 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 21.1MB/s]\n",
      "2023-10-21 19:53:52 INFO: Loading these models for language: ru (Russian):\n",
      "==============================\n",
      "| Processor | Package        |\n",
      "------------------------------\n",
      "| tokenize  | taiga          |\n",
      "| pos       | taiga_charlm   |\n",
      "| lemma     | taiga_nocharlm |\n",
      "| depparse  | taiga_charlm   |\n",
      "==============================\n",
      "\n",
      "2023-10-21 19:53:52 INFO: Using device: cpu\n",
      "2023-10-21 19:53:52 INFO: Loading: tokenize\n",
      "2023-10-21 19:53:52 INFO: Loading: pos\n",
      "2023-10-21 19:53:53 INFO: Loading: lemma\n",
      "2023-10-21 19:53:53 INFO: Loading: depparse\n",
      "2023-10-21 19:53:53 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('ru')\n",
    "nlp = stanza.Pipeline(\"ru\", package=\"Taiga\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38883b35-034a-469b-b50f-d73c190b291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanza.utils.conll import CoNLL\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c5c14-94bf-4b24-985f-ce0cc3a437a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8ef3265-bf35-4a28-a95c-ef1ec873d39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"conllu/connlu_with_meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b70bf00f-2a9a-4792-b1e4-850cd6c4e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# конвертируем .txt в .conllu, попутно пропарсив текст Stanz'ой\n",
    "def conllu_vert(filename):\n",
    "    with open(filename, 'r', encoding = 'UTF-8') as f: \n",
    "        fread = f.readlines()\n",
    "        metadata = fread[0]\n",
    "        full_text = ''\n",
    "        for n in range(2, len(fread)-1):\n",
    "            full_text = full_text + fread[n]\n",
    "    full_text = re.sub(r\"[\\u00AD]\", \"\", full_text)\n",
    "    doc = nlp(full_text)\n",
    "    conllu_fname = os.path.join('conllu', filename.split('.')[0]+'.conllu')\n",
    "    metadata = json.loads(metadata)\n",
    "    with open(conllu_fname, \"w\", encoding = \"UTF-8\") as f:\n",
    "        f.write(f\"# metadata = Название: {metadata['fic_name']}, автор: {metadata['author']}, ссылка: {metadata['link']}\\n\")\n",
    "    CoNLL.write_doc2conll(doc, conllu_fname, mode=\"a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c2d4c6f3-915a-452f-b6dd-3dfdb126471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\GL_24_12_2019\\\\YandexDisk\\\\NLP_hws\\\\\\\\better_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b4ab5c03-b165-4f79-adbb-13fd80fe5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = []\n",
    "for file in os.listdir():\n",
    "    if file.endswith('.txt'):\n",
    "        files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3d5568c0-710b-45cb-80ed-846731cbe24f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in files: \n",
    "    conllu_vert(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2d87fb87-e341-4c74-8215-5dd9d9df06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0fedb848-1fc5-4d18-9aee-c06a6e7200fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"merged.txt\", \"wb\") as result_file:\n",
    "    for f in files:\n",
    "        with open(f, \"rb\") as file:\n",
    "            result_file.write(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cb102920-4663-4788-99ca-acdec271d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('merged.txt', 'r', encoding='UTF-8') as file:\n",
    "    text = file.read()\n",
    "    smth = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6dbd009c-9451-4db5-ac1a-11d7d6912ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего токенов в нашем корпусе приблизительно 228129\n"
     ]
    }
   ],
   "source": [
    "tokens_quantity = [token for token in smth if token not in punctuation]\n",
    "print(f\"Всего токенов в нашем корпусе приблизительно {len(tokens_quantity)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0bc12a8a-a12a-4847-a03f-242f46322576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# занимаемся добавлением метадаты формата название-автор-ссылка в conllu файлы\n",
    "def add_metadata(file):\n",
    "    with open(file, 'r', encoding=\"UTF-8\") as f:\n",
    "        txt = f.readlines()\n",
    "        meta = txt[0]\n",
    "        f.seek(0)\n",
    "        txt = f.readlines()[1:]\n",
    "        meta_fname = os.path.join('conllu_with_meta', file.split('.')[0]+'.conllu')\n",
    "        with open(meta_fname, 'w', encoding=\"UTF-8\") as fw:\n",
    "            for lines in txt:\n",
    "                if lines[0:9] != '# sent_id':\n",
    "                    fw.write(lines)\n",
    "                elif lines[0:9] == '# sent_id':\n",
    "                    fw.write(lines)\n",
    "                    fw.write(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7f801b13-99c9-443d-b053-de50c461f83d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\"conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a5127533-1117-4f42-ad27-6c12b7bd1a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for file in os.listdir():\n",
    "    if file.endswith('.conllu'):\n",
    "        files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4a46cc53-15fc-4a28-872c-cf9fd08a0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    add_metadata(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900f83e-8029-464d-813f-e049a2865b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
