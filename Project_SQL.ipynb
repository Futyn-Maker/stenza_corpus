{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. База данных для хранения исходных данных, промежуточных и окончательных результатов.\n",
    "Создано в DB Browser for SQLite:\n",
    "\n",
    "CREATE TABLE \"Sentences\" (\n",
    "\t\"ID\"\tINTEGER,\n",
    "\t\"Sentence\"\tTEXT,\n",
    "\t\"Metadata\"\tTEXT,\n",
    "\tPRIMARY KEY(\"ID\" AUTOINCREMENT)\n",
    ")\n",
    "CREATE TABLE \"Words\" (\n",
    "\t\"ID\"\tINTEGER,\n",
    "\t\"Token\"\tTEXT,\n",
    "\t\"Lemma\"\tTEXT,\n",
    "\t\"POS\"\tTEXT,\n",
    "\t\"ID_sent\"\tINTEGER,\n",
    "\tPRIMARY KEY(\"ID\" AUTOINCREMENT)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from conllu import parse\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('DoctorWho.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для коллекции предложений conllu идёт первичная итерация по предложениям sent с метадатой meta, вторичная итерация по токенам token с леммой lemma и тегом tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пишем данные в базу\n",
    "id_sent = 1\n",
    "id_token = 1\n",
    "for file in listdir('fics_conllus'):\n",
    "    filename = 'fics_conllus/' + file\n",
    "    with open(filename) as f:\n",
    "        text = f.read()\n",
    "    tokens = parse(text)\n",
    "    for line in tokens:\n",
    "        meta = str(line.metadata['metadata'])\n",
    "        sent = str(line.metadata['text'])\n",
    "        cur.execute(\"INSERT INTO Sentences VALUES (?,?,?)\", (id_sent, sent, meta)) # id, предложение, метадата\n",
    "        for word in line:\n",
    "            token = word['form'].lower()\n",
    "            lemma = word['lemma']\n",
    "            tag = word['upos']\n",
    "            if tag != 'PUNCT':\n",
    "                cur.execute(\n",
    "                    \"\"\"\n",
    "                        INSERT INTO Words\n",
    "                        VALUES (?,?,?,?,?)\n",
    "                    \"\"\", (id_token,token,lemma,tag,id_sent)) # id, токен, лемма, тег, id предложения, из которго взято слово\n",
    "                id_token +=1\n",
    "        id_sent += 1\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request = [([entry], search_type)], search_type IN token, lemma, POS, lemma+POS\n",
    "# Если нужна последовательность из 2-3 слов\n",
    "# Вынимаем из базы  ID предложений, в которых встречаются искомые вхождения, и ID самих вхождений\n",
    "def search_sequence(request):\n",
    "    output = []\n",
    "    for entry in request:\n",
    "        if entry[1] != 'lemma+pos':\n",
    "            query = 'SELECT ID_sent, ID FROM Words WHERE Words.'+ entry[1]+ '=\"'  + entry[0][0] +'\"'\n",
    "        else:\n",
    "            query = 'SELECT ID_sent, ID FROM Words WHERE Words.lemma=\"'  + entry[0][0] + '\"AND Words.POS=\"' + entry[0][1]+'\"'\n",
    "        res = cur.execute(query)\n",
    "        output.append(res.fetchall())\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Продолжение поиска последовательности из 2-3 слов\n",
    "# Из предыдущей функции мы получили для каждого запроса список пар (ID предложений, в которых встречаются искомые вхождения, [ID самих вхождений])\n",
    "# и сложили их в список второго уровня, из двух или трёх элементов. Собираем теперь словарь нужных нам предложений. \n",
    "def merge_sequence(one, two, first):\n",
    "    res = {}\n",
    "    second = {}\n",
    "    if not first:\n",
    "        for entry in one:\n",
    "            if entry[0] not in first.keys():\n",
    "                first[entry[0]] = [entry[1]+1]\n",
    "            else:\n",
    "                first[entry[0]].append(entry[1]+1)\n",
    "    else:\n",
    "        for key in first:\n",
    "            first[key] = [value + 1 for value in first[key]]\n",
    "    for entry in two:\n",
    "        if entry[0] in first.keys():\n",
    "            if entry[0] not in second.keys():\n",
    "                second[entry[0]] = [entry[1]]\n",
    "            else:\n",
    "                second[entry[0]].append(entry[1])\n",
    "    for key in second.keys():\n",
    "        if set(first[key]) & set(second[key]):\n",
    "            res[key] = list(set(first[key]) & set(second[key]))\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request = [([entry], search_type)], search_type IN token, lemma, POS, lemma+POS\n",
    "# Если нужно одно слово - сразу вынимаем из базы предложения и метадату\n",
    "def search_one(request):\n",
    "    if request[0][1] != 'lemma+pos':\n",
    "        query = 'SELECT DISTINCT Sentence, Metadata FROM Sentences JOIN Words ON Words.ID_sent = Sentences.ID WHERE Words.' + request[0][1]+ '=\"' + request[0][0][0]+'\"'\n",
    "    else:\n",
    "        query = 'SELECT DISTINCT Sentence, Metadata FROM Sentences JOIN Words ON Words.ID_sent = Sentences.ID WHERE Words.lemma' + '=\"' + request[0][0][0]+ '\" AND Words.POS' + '=\"' + request[0][0][1] + '\"'\n",
    "    res = cur.execute(query)\n",
    "    result = res.fetchall()\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request = [([entry], search_type)], search_type IN token, lemma, POS, lemma+POS\n",
    "def main(request):\n",
    "    if len(request) == 1:\n",
    "        output = search_one(request)\n",
    "    else:\n",
    "        ids = search_sequence(request)\n",
    "        l = len(ids)\n",
    "        flag = True\n",
    "        for i in range(l-1):\n",
    "            if flag:\n",
    "                result = merge_sequence(ids[i], ids[i+1], {})\n",
    "                flag = False\n",
    "            else:\n",
    "                result = merge_sequence([],ids[i+1],result)\n",
    "        if result:\n",
    "            keys = list(result.keys())\n",
    "            output = []\n",
    "            for key in keys:\n",
    "                cur.execute(\"SELECT Sentence, Metadata FROM Sentences WHERE ID = ?\", (key,))\n",
    "                output.append(cur.fetchone())\n",
    "        else:\n",
    "            output = [(\"Sorry, no matches to your request\")]\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of functions\n",
    "# requests\n",
    "lemma_search = [(['он'], 'lemma')]\n",
    "token_search = [(['его'], 'token')]\n",
    "pos_search = [(['PROPN'], 'POS')]\n",
    "lemma_pos_search = [(['доктор','PROPN'], 'lemma+pos')]\n",
    "# long_search = [(['не'], 'lemma'), (['знать'], 'lemma'), (['что'], 'lemma')]\n",
    "long_search = [(['ADJ'], 'POS'), (['NOUN'], 'POS'), (['VERB'], 'POS'), (['NOUN'], 'POS')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(token_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
