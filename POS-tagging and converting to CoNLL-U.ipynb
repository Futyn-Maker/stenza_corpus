{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa36d12f-0a28-4881-9f2a-f32fa015a3ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in c:\\python38\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: emoji in c:\\python38\\lib\\site-packages (from stanza) (2.8.0)\n",
      "Requirement already satisfied: numpy in c:\\python38\\lib\\site-packages (from stanza) (1.24.2)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\python38\\lib\\site-packages (from stanza) (3.17.3)\n",
      "Requirement already satisfied: requests in c:\\python38\\lib\\site-packages (from stanza) (2.26.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\python38\\lib\\site-packages (from stanza) (2.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\python38\\lib\\site-packages (from stanza) (4.64.0)\n",
      "Requirement already satisfied: six>=1.9 in c:\\python38\\lib\\site-packages (from protobuf>=3.15.0->stanza) (1.15.0)\n",
      "Requirement already satisfied: filelock in c:\\python38\\lib\\site-packages (from torch>=1.3.0->stanza) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in c:\\python38\\lib\\site-packages (from torch>=1.3.0->stanza) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\python38\\lib\\site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\python38\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\python38\\lib\\site-packages (from torch>=1.3.0->stanza) (3.0.1)\n",
      "Requirement already satisfied: fsspec in c:\\python38\\lib\\site-packages (from torch>=1.3.0->stanza) (2023.9.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python38\\lib\\site-packages (from requests->stanza) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python38\\lib\\site-packages (from requests->stanza) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python38\\lib\\site-packages (from requests->stanza) (2.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python38\\lib\\site-packages (from requests->stanza) (3.2)\n",
      "Requirement already satisfied: colorama in c:\\python38\\lib\\site-packages (from tqdm->stanza) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python38\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python38\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a05fcbb1-d4a1-4e9f-b51d-441fe4ad20f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 20.9MB/s]\n",
      "2023-10-18 18:06:07 INFO: Downloading default packages for language: ru (Russian) ...\n",
      "2023-10-18 18:06:08 INFO: File exists: C:\\Users\\GL_24_12_2019\\stanza_resources\\ru\\default.zip\n",
      "2023-10-18 18:06:11 INFO: Finished downloading models and saved to C:\\Users\\GL_24_12_2019\\stanza_resources.\n",
      "2023-10-18 18:06:11 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 10.5MB/s]\n",
      "2023-10-18 18:06:13 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| pos       | syntagrus_charlm   |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "| depparse  | syntagrus_charlm   |\n",
      "| ner       | wikiner            |\n",
      "==================================\n",
      "\n",
      "2023-10-18 18:06:13 INFO: Using device: cpu\n",
      "2023-10-18 18:06:13 INFO: Loading: tokenize\n",
      "2023-10-18 18:06:13 INFO: Loading: pos\n",
      "2023-10-18 18:06:13 INFO: Loading: lemma\n",
      "2023-10-18 18:06:13 INFO: Loading: depparse\n",
      "2023-10-18 18:06:13 INFO: Loading: ner\n",
      "2023-10-18 18:06:14 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('ru') # download English model\n",
    "nlp = stanza.Pipeline('ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "38883b35-034a-469b-b50f-d73c190b291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanza.utils.conll import CoNLL\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "522c5c14-94bf-4b24-985f-ce0cc3a437a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b70bf00f-2a9a-4792-b1e4-850cd6c4e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# конвертируем .txt в .conllu, попутно пропарсив текст Stanz'ой\n",
    "def conllu_vert(filename):\n",
    "    with open(filename, 'r', encoding = 'UTF-8') as f: \n",
    "        fread = f.readlines()\n",
    "        metadata = fread[0]\n",
    "        full_text = ''\n",
    "        for n in range(2, len(fread)-1):\n",
    "            full_text = full_text + fread[n]\n",
    "    full_text = re.sub(r\"[\\u00AD]\", \"\", full_text)\n",
    "    doc = nlp(full_text)\n",
    "    conllu_fname = os.path.join('conllu', filename.split('.')[0]+'.conllu')\n",
    "    metadata = json.loads(metadata)\n",
    "    with open(conllu_fname, \"w\", encoding = \"UTF-8\") as f:\n",
    "        f.write(f\"# fic_name: {metadata['fic_name']}\\n\")\n",
    "        f.write(f\"# author: {metadata['author']}\\n\")\n",
    "        f.write(f\"# link: {metadata['link']}\\n\")\n",
    "    CoNLL.write_doc2conll(doc, conllu_fname, mode=\"a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab5c03-b165-4f79-adbb-13fd80fe5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = []\n",
    "for file in os.listdir():\n",
    "    if file.endswith('.txt'):\n",
    "        files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3d5568c0-710b-45cb-80ed-846731cbe24f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in files: \n",
    "    conllu_vert(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d87fb87-e341-4c74-8215-5dd9d9df06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fedb848-1fc5-4d18-9aee-c06a6e7200fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"merged.txt\", \"wb\") as result_file:\n",
    "    for f in files:\n",
    "        with open(f, \"rb\") as file:\n",
    "            result_file.write(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb102920-4663-4788-99ca-acdec271d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('merged.txt', 'r', encoding='UTF-8') as file:\n",
    "    text = file.read()\n",
    "    smth = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6dbd009c-9451-4db5-ac1a-11d7d6912ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего токенов в нашем корпусе приблизительно 288594\n"
     ]
    }
   ],
   "source": [
    "tokens_quantity = [token for token in smth if token not in punctuation]\n",
    "print(f\"Всего токенов в нашем корпусе приблизительно {len(tokens_quantity)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
